{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87b2f5b2",
   "metadata": {},
   "source": [
    "# POS Tagging, n - grams(uni/bi/tri), Naive Bayes Classifier\n",
    "## Prathamesh Patil\n",
    "### 09/22/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c81adcc",
   "metadata": {},
   "source": [
    "#### Task 1: Load the dataset \n",
    "- Import the training set and a test set (csv files).\n",
    "    - Note: The data has already been preprocessed. No additional preprocessing is expected. \n",
    "- List down the number of reviews in the training set and the test set.\n",
    "- Remove the reviews that have blank (empty) CleanedReview from both training set and the test sets.\n",
    "- List down the number of reviews in the training set and the test set after removing empty values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "983aaceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcec8b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13000, 20)\n"
     ]
    }
   ],
   "source": [
    "# Read train corpus from a csv file into a dataframe and see the shapes and data types of the columns in the dataset.\n",
    "df_train = pd.read_csv('Books_preprocessed_train_data.csv', skipinitialspace = True)\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1173e5bf",
   "metadata": {},
   "source": [
    "There are **13000** instances in training dataset along with **20** attributes for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b03a73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1602, 20)\n"
     ]
    }
   ],
   "source": [
    "# Read test corpus from a csv file into a dataframe and see the shapes and data types of the columns in the dataset.\n",
    "\n",
    "df_test = pd.read_csv('Books_preprocessed_test_data.csv',skipinitialspace = True)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54479891",
   "metadata": {},
   "source": [
    "There are **1602** instances in training dataset along with **12** attributes for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86da46a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['CleanedReview'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f760e790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['CleanedReview'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a82ce5b",
   "metadata": {},
   "source": [
    "Since there are only **4** null values out of **13,000** instances, dropping these would be most sensible. This won't affect our analysis. But if there had been more values, then we would have filled these values with NA or string - 'No review available'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d587bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping null values\n",
    "\n",
    "df_train = df_train[pd.notnull(df_train['CleanedReview'])]\n",
    "df_train = df_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06b7d23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['CleanedReview'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d753d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12996, 20)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06469d",
   "metadata": {},
   "source": [
    "- There are **12996** instances in **training** dataset along with **20** attributes for each instance.\n",
    "- There are **1602** instances in **testing** dataset along with **20** attributes for each instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e61db76",
   "metadata": {},
   "source": [
    "#### Task 2: POS Tagging\n",
    "\n",
    "-  Make a copy of the training data for Task 2. Implement the below steps on this copy to refrain from editing the actual training data that will be used for further tasks.\n",
    "- Using a package of your choice (e.g. NLTK in Python), perform part-of-speech (POS) tagging of the words. \n",
    "    - Input:\n",
    "        - TokenizedReview: ['this', 'book', 'is', 'super', 'annoying', 'to', 'read', 'it', 's', 'so', \n",
    "        'repetitive']\n",
    "    - Expected Output:\n",
    "        - PosTaggedReview: [[('this', ['DT']), ('book', ['NN']), ('is', ['VBZ']), ('super', ['JJ']), \n",
    "        ('annoying', ['VBG']), ('to', ['TO']), ('read', ['VB']), ('it', ['PRP']), ('s', ['PRP']), ('so', \n",
    "        ['RB']), ('repetitive', ['JJ'])]]\n",
    "- Explain what parts of speech could be useful for sentiment analysis and why?\n",
    "- Report the POS-tagging results for 3 examples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "770af6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Nemo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d7f8b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6c1e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posTagger(review):\n",
    "    wordlist = eval(review)\n",
    "    tokens_tag = pos_tag(wordlist) \n",
    "    return tokens_tag\n",
    "\n",
    "df_train_copy[\"PosTaggedReview\"] = df_train_copy.apply(lambda row : posTagger(row[\"TokenizedReview\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28991e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>...</th>\n",
       "      <th>image</th>\n",
       "      <th>OriginalReview</th>\n",
       "      <th>Mentions</th>\n",
       "      <th>CleanedReview</th>\n",
       "      <th>TokenizedReview</th>\n",
       "      <th>StopwordRemovedReview</th>\n",
       "      <th>StemmedReview</th>\n",
       "      <th>BiGrams</th>\n",
       "      <th>ProcessedReview</th>\n",
       "      <th>PosTaggedReview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>09 18, 2006</td>\n",
       "      <td>A294QSAEH1Z7YI</td>\n",
       "      <td>0001713353</td>\n",
       "      <td>{'Format:': ' Hardcover'}</td>\n",
       "      <td>BHGobuchul</td>\n",
       "      <td>41 years later:\\n\\nThe cheese is government ch...</td>\n",
       "      <td>Outdated, but a good 1960s primer</td>\n",
       "      <td>1158537600</td>\n",
       "      <td>...</td>\n",
       "      <td>are</td>\n",
       "      <td>41 years later:\\n\\nthe cheese is government ch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41 years later the cheese is government cheese...</td>\n",
       "      <td>['41', 'years', 'later', 'the', 'cheese', 'is'...</td>\n",
       "      <td>['41', 'years', 'later', 'cheese', 'government...</td>\n",
       "      <td>['41', 'year', 'later', 'the', 'chees', 'is', ...</td>\n",
       "      <td>[('41', 'years'), ('years', 'later:'), ('later...</td>\n",
       "      <td>41 year later the chees is govern chees the mi...</td>\n",
       "      <td>[(41, CD), (years, NNS), (later, RB), (the, DT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>True</td>\n",
       "      <td>03 1, 2015</td>\n",
       "      <td>A3ZG0U3FOF3T1</td>\n",
       "      <td>0001061240</td>\n",
       "      <td>{'Format:': ' Hardcover'}</td>\n",
       "      <td>P. Howell</td>\n",
       "      <td>Looking for a Louis Untermeyer book  from the ...</td>\n",
       "      <td>Two Stars</td>\n",
       "      <td>1425168000</td>\n",
       "      <td>...</td>\n",
       "      <td>are</td>\n",
       "      <td>looking for a louis untermeyer book  from the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>looking for a louis untermeyer book from the 1...</td>\n",
       "      <td>['looking', 'for', 'a', 'louis', 'untermeyer',...</td>\n",
       "      <td>['looking', 'louis', 'untermeyer', 'book', '19...</td>\n",
       "      <td>['look', 'for', 'a', 'loui', 'untermey', 'book...</td>\n",
       "      <td>[('looking', 'for'), ('for', 'a'), ('a', 'loui...</td>\n",
       "      <td>look for a loui untermey book from the 1980 an...</td>\n",
       "      <td>[(looking, VBG), (for, IN), (a, DT), (louis, J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>05 18, 2002</td>\n",
       "      <td>AJ8AQG2X9JJ2Y</td>\n",
       "      <td>0001712799</td>\n",
       "      <td>{'Format:': ' School &amp; Library Binding'}</td>\n",
       "      <td>Donald Gillies</td>\n",
       "      <td>Dr. Seuss has some really brilliant books.  Th...</td>\n",
       "      <td>A below-average Dr. Seuss Book</td>\n",
       "      <td>1021680000</td>\n",
       "      <td>...</td>\n",
       "      <td>are</td>\n",
       "      <td>dr. seuss has some really brilliant books.  th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dr seuss has some really brilliant books this ...</td>\n",
       "      <td>['dr', 'seuss', 'has', 'some', 'really', 'bril...</td>\n",
       "      <td>['dr', 'seuss', 'really', 'brilliant', 'books'...</td>\n",
       "      <td>['dr', 'seuss', 'ha', 'some', 'realli', 'brill...</td>\n",
       "      <td>[('dr.', 'seuss'), ('seuss', 'has'), ('has', '...</td>\n",
       "      <td>dr seuss ha some realli brilliant book thi boo...</td>\n",
       "      <td>[(dr, NN), (seuss, NN), (has, VBZ), (some, DT)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>True</td>\n",
       "      <td>02 20, 2016</td>\n",
       "      <td>A2M08SO0PJKPAV</td>\n",
       "      <td>0001712799</td>\n",
       "      <td>{'Format:': ' Hardcover'}</td>\n",
       "      <td>Emily</td>\n",
       "      <td>Completly boring!!! Yes it's a childerns book ...</td>\n",
       "      <td>Don't waste your money</td>\n",
       "      <td>1455926400</td>\n",
       "      <td>...</td>\n",
       "      <td>are</td>\n",
       "      <td>completly boring!!! yes it's a childerns book ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>completly boring yes it s a childerns book tha...</td>\n",
       "      <td>['completly', 'boring', 'yes', 'it', 's', 'a',...</td>\n",
       "      <td>['completly', 'boring', 'yes', 'childerns', 'b...</td>\n",
       "      <td>['completli', 'bore', 'ye', 'it', 's', 'a', 'c...</td>\n",
       "      <td>[('completly', 'boring!!!'), ('boring!!!', 'ye...</td>\n",
       "      <td>completli bore ye it s a childern book that th...</td>\n",
       "      <td>[(completly, RB), (boring, VBG), (yes, NNS), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>False</td>\n",
       "      <td>07 8, 2004</td>\n",
       "      <td>A1JS302JFHH9DJ</td>\n",
       "      <td>0002006448</td>\n",
       "      <td>{'Format:': ' Hardcover'}</td>\n",
       "      <td>Daniel H. Bigelow</td>\n",
       "      <td>The Carpet Wars is a sampler of informal writi...</td>\n",
       "      <td>Painless Education</td>\n",
       "      <td>1089244800</td>\n",
       "      <td>...</td>\n",
       "      <td>are</td>\n",
       "      <td>the carpet wars is a sampler of informal writi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the carpet wars is a sampler of informal writi...</td>\n",
       "      <td>['the', 'carpet', 'wars', 'is', 'a', 'sampler'...</td>\n",
       "      <td>['carpet', 'wars', 'sampler', 'informal', 'wri...</td>\n",
       "      <td>['the', 'carpet', 'war', 'is', 'a', 'sampler',...</td>\n",
       "      <td>[('the', 'carpet'), ('carpet', 'wars'), ('wars...</td>\n",
       "      <td>the carpet war is a sampler of inform write fr...</td>\n",
       "      <td>[(the, DT), (carpet, NN), (wars, NNS), (is, VB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    overall  verified   reviewTime      reviewerID        asin  \\\n",
       "0   neutral     False  09 18, 2006  A294QSAEH1Z7YI  0001713353   \n",
       "1  negative      True   03 1, 2015   A3ZG0U3FOF3T1  0001061240   \n",
       "2   neutral     False  05 18, 2002   AJ8AQG2X9JJ2Y  0001712799   \n",
       "3  negative      True  02 20, 2016  A2M08SO0PJKPAV  0001712799   \n",
       "4   neutral     False   07 8, 2004  A1JS302JFHH9DJ  0002006448   \n",
       "\n",
       "                                      style       reviewerName  \\\n",
       "0                 {'Format:': ' Hardcover'}         BHGobuchul   \n",
       "1                 {'Format:': ' Hardcover'}          P. Howell   \n",
       "2  {'Format:': ' School & Library Binding'}     Donald Gillies   \n",
       "3                 {'Format:': ' Hardcover'}              Emily   \n",
       "4                 {'Format:': ' Hardcover'}  Daniel H. Bigelow   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0  41 years later:\\n\\nThe cheese is government ch...   \n",
       "1  Looking for a Louis Untermeyer book  from the ...   \n",
       "2  Dr. Seuss has some really brilliant books.  Th...   \n",
       "3  Completly boring!!! Yes it's a childerns book ...   \n",
       "4  The Carpet Wars is a sampler of informal writi...   \n",
       "\n",
       "                             summary  unixReviewTime  ... image  \\\n",
       "0  Outdated, but a good 1960s primer      1158537600  ...   are   \n",
       "1                          Two Stars      1425168000  ...   are   \n",
       "2     A below-average Dr. Seuss Book      1021680000  ...   are   \n",
       "3             Don't waste your money      1455926400  ...   are   \n",
       "4                 Painless Education      1089244800  ...   are   \n",
       "\n",
       "                                      OriginalReview Mentions  \\\n",
       "0  41 years later:\\n\\nthe cheese is government ch...      NaN   \n",
       "1  looking for a louis untermeyer book  from the ...      NaN   \n",
       "2  dr. seuss has some really brilliant books.  th...      NaN   \n",
       "3  completly boring!!! yes it's a childerns book ...      NaN   \n",
       "4  the carpet wars is a sampler of informal writi...      NaN   \n",
       "\n",
       "                                       CleanedReview  \\\n",
       "0  41 years later the cheese is government cheese...   \n",
       "1  looking for a louis untermeyer book from the 1...   \n",
       "2  dr seuss has some really brilliant books this ...   \n",
       "3  completly boring yes it s a childerns book tha...   \n",
       "4  the carpet wars is a sampler of informal writi...   \n",
       "\n",
       "                                     TokenizedReview  \\\n",
       "0  ['41', 'years', 'later', 'the', 'cheese', 'is'...   \n",
       "1  ['looking', 'for', 'a', 'louis', 'untermeyer',...   \n",
       "2  ['dr', 'seuss', 'has', 'some', 'really', 'bril...   \n",
       "3  ['completly', 'boring', 'yes', 'it', 's', 'a',...   \n",
       "4  ['the', 'carpet', 'wars', 'is', 'a', 'sampler'...   \n",
       "\n",
       "                               StopwordRemovedReview  \\\n",
       "0  ['41', 'years', 'later', 'cheese', 'government...   \n",
       "1  ['looking', 'louis', 'untermeyer', 'book', '19...   \n",
       "2  ['dr', 'seuss', 'really', 'brilliant', 'books'...   \n",
       "3  ['completly', 'boring', 'yes', 'childerns', 'b...   \n",
       "4  ['carpet', 'wars', 'sampler', 'informal', 'wri...   \n",
       "\n",
       "                                       StemmedReview  \\\n",
       "0  ['41', 'year', 'later', 'the', 'chees', 'is', ...   \n",
       "1  ['look', 'for', 'a', 'loui', 'untermey', 'book...   \n",
       "2  ['dr', 'seuss', 'ha', 'some', 'realli', 'brill...   \n",
       "3  ['completli', 'bore', 'ye', 'it', 's', 'a', 'c...   \n",
       "4  ['the', 'carpet', 'war', 'is', 'a', 'sampler',...   \n",
       "\n",
       "                                             BiGrams  \\\n",
       "0  [('41', 'years'), ('years', 'later:'), ('later...   \n",
       "1  [('looking', 'for'), ('for', 'a'), ('a', 'loui...   \n",
       "2  [('dr.', 'seuss'), ('seuss', 'has'), ('has', '...   \n",
       "3  [('completly', 'boring!!!'), ('boring!!!', 'ye...   \n",
       "4  [('the', 'carpet'), ('carpet', 'wars'), ('wars...   \n",
       "\n",
       "                                     ProcessedReview  \\\n",
       "0  41 year later the chees is govern chees the mi...   \n",
       "1  look for a loui untermey book from the 1980 an...   \n",
       "2  dr seuss ha some realli brilliant book thi boo...   \n",
       "3  completli bore ye it s a childern book that th...   \n",
       "4  the carpet war is a sampler of inform write fr...   \n",
       "\n",
       "                                     PosTaggedReview  \n",
       "0  [(41, CD), (years, NNS), (later, RB), (the, DT...  \n",
       "1  [(looking, VBG), (for, IN), (a, DT), (louis, J...  \n",
       "2  [(dr, NN), (seuss, NN), (has, VBZ), (some, DT)...  \n",
       "3  [(completly, RB), (boring, VBG), (yes, NNS), (...  \n",
       "4  [(the, DT), (carpet, NN), (wars, NNS), (is, VB...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a18342",
   "metadata": {},
   "source": [
    "*I think verbs,noun, adverb and adjectives are the most important part of speech for sentiment analysis. Majority of the sentiments are hidden in these 4 parts of speech.* \n",
    "\n",
    "*For instance, verbs like love,hate,enjoy can tell you more about feelings. Similarly, adverbs like beautifully, sadly, happily or angrily carry a lot of weight in the sentence. In addition, adjevctives like awesome, superb, disgusting will help in analyzing emotions of reviewer. Lastly, nouns will have least immportance of these 4 parts of speech because there will be words like boy, book, character, character Name, etc that majorly lie in neutral category.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "753df05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Review 1----\n",
      "\n",
      "TokenizedReview: ['41', 'years', 'later', 'the', 'cheese', 'is', 'government', 'cheese', 'the', 'mice', 'objected', 'to', 'the', 'king', 's', 'idea', 'of', 'good', 'manners', 'as', 'species', 'centric', 'and', 'rebelled', 'the', 'king', 'blamed', 'the', 'peasants', 'and', 'forbade', 'them', 'to', 'keep', 'cats', 'or', 'chase', 'mice', 'from', 'their', 'homes', 'this', 'made', 'things', 'worse', 'peasants', 'that', 'could', 'afford', 'to', 'do', 'so', 'moved', 'as', 'far', 'away', 'from', 'mice', 'as', 'possible', 'i', 'can', 't', 'wait', 'for', 'the', 'next', 'chapter']\n",
      "\n",
      "\n",
      "PosTaggedReview: [('41', 'CD'), ('years', 'NNS'), ('later', 'RB'), ('the', 'DT'), ('cheese', 'NN'), ('is', 'VBZ'), ('government', 'NN'), ('cheese', 'VBG'), ('the', 'DT'), ('mice', 'NN'), ('objected', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('king', 'NN'), ('s', 'JJ'), ('idea', 'NN'), ('of', 'IN'), ('good', 'JJ'), ('manners', 'NNS'), ('as', 'IN'), ('species', 'NNS'), ('centric', 'VBP'), ('and', 'CC'), ('rebelled', 'VBD'), ('the', 'DT'), ('king', 'NN'), ('blamed', 'VBD'), ('the', 'DT'), ('peasants', 'NNS'), ('and', 'CC'), ('forbade', 'VB'), ('them', 'PRP'), ('to', 'TO'), ('keep', 'VB'), ('cats', 'NNS'), ('or', 'CC'), ('chase', 'VB'), ('mice', 'NN'), ('from', 'IN'), ('their', 'PRP$'), ('homes', 'NNS'), ('this', 'DT'), ('made', 'VBN'), ('things', 'NNS'), ('worse', 'JJR'), ('peasants', 'NNS'), ('that', 'WDT'), ('could', 'MD'), ('afford', 'VB'), ('to', 'TO'), ('do', 'VB'), ('so', 'RB'), ('moved', 'VBN'), ('as', 'IN'), ('far', 'RB'), ('away', 'RB'), ('from', 'IN'), ('mice', 'NNS'), ('as', 'IN'), ('possible', 'JJ'), ('i', 'NN'), ('can', 'MD'), ('t', 'VB'), ('wait', 'NN'), ('for', 'IN'), ('the', 'DT'), ('next', 'JJ'), ('chapter', 'NN')]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"----Review 1----\\n\")\n",
    "print(\"TokenizedReview:\",df_train_copy.loc[0][\"TokenizedReview\"])\n",
    "print(\"\\n\")\n",
    "print(\"PosTaggedReview:\", df_train_copy.loc[0][\"PosTaggedReview\"])\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b49338e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Review 2----\n",
      "\n",
      "TokenizedReview: ['the', 'carpet', 'wars', 'is', 'a', 'sampler', 'of', 'informal', 'writing', 'from', 'australian', 'journalist', 'and', 'avid', 'carpet', 'collector', 'christopher', 'kremmer', 'over', 'ten', 'years', 'in', 'central', 'asia', 'since', 'most', 'of', 'it', 'was', 'written', 'and', 'concerns', 'events', 'before', '9', '11', 'when', 'the', 'area', 'was', 'not', 'established', 'in', 'the', 'west', 's', 'cultural', 'radar', 'as', 'it', 'is', 'today', 'it', 'gives', 'a', 'view', 'of', 'the', 'region', 'that', 'is', 'uncluttered', 'by', 'hindsight', 'reevaluations', 'kremmer', 'writes', 'of', 'his', 'time', 'in', 'afghanistan', 'pakistan', 'tajikstan', 'kashmir', 'and', 'iran', 'giving', 'us', 'colorful', 'and', 'non', 'journalistic', 'slices', 'of', 'life', 'from', 'each', 'region', 'he', 'enlivens', 'his', 'writings', 'with', 'vivid', 'character', 'studies', 'of', 'those', 'he', 'met', 'on', 'his', 'travels', 'from', 'dignitaries', 'like', 'ill', 'fated', 'afghan', 'dictator', 'mohammed', 'najibullah', 'and', 'legendary', 'guerilla', 'ahmad', 'shah', 'massoud', 'to', 'various', 'carpet', 'dealers', 'kremmer', 'got', 'to', 'know', 'over', 'his', 'time', 'in', 'the', 'region', 'between', 'these', 'character', 'sketches', 'and', 'kremmer', 's', 'anecdotes', 'he', 'delivers', 'measured', 'doses', 'of', 'regional', 'history', 'and', 'politics', 'and', 'he', 'imparts', 'a', 'surprising', 'amount', 'of', 'information', 'about', 'his', 'favorite', 'hobby', 'the', 'asian', 'carpet', 'the', 'result', 'is', 'more', 'than', 'just', 'some', 'very', 'entertaining', 'travel', 'writing', 'kremmer', 's', 'lively', 'and', 'discursive', 'work', 'also', 'functions', 'as', 'an', 'excellent', 'introduction', 'to', 'the', 'central', 'asian', 'economy', 'and', 'politics', 'besides', 'being', 'for', 'those', 'who', 'just', 'like', 'to', 'read', 'about', 'travel', 'in', 'interesting', 'foreign', 'parts', 'the', 'carpet', 'wars', 'will', 'also', 'be', 'useful', 'for', 'non', 'scholars', 'who', 'want', 'to', 'have', 'some', 'idea', 'how', 'movements', 'like', 'the', 'taliban', 'came', 'to', 'be', 'but', 'want', 'to', 'take', 'a', 'spoonful', 'of', 'sugar', 'with', 'this', 'medicine', 'kremmer', 's', 'book', 'also', 'taught', 'me', 'that', 'i', 'll', 'never', 'know', 'enough', 'to', 'bargain', 'effectively', 'for', 'an', 'asian', 'carpet', 'but', 'his', 'rueful', 'and', 'wry', 'work', 'also', 'admits', 'that', 'there', 'is', 'a', 'certain', 'pleasure', 'in', 'being', 'cheated']\n",
      "\n",
      "\n",
      "PosTaggedReview: [('the', 'DT'), ('carpet', 'NN'), ('wars', 'NNS'), ('is', 'VBZ'), ('a', 'DT'), ('sampler', 'NN'), ('of', 'IN'), ('informal', 'JJ'), ('writing', 'VBG'), ('from', 'IN'), ('australian', 'JJ'), ('journalist', 'NN'), ('and', 'CC'), ('avid', 'JJ'), ('carpet', 'NN'), ('collector', 'NN'), ('christopher', 'NN'), ('kremmer', 'NN'), ('over', 'IN'), ('ten', 'JJ'), ('years', 'NNS'), ('in', 'IN'), ('central', 'JJ'), ('asia', 'NN'), ('since', 'IN'), ('most', 'JJS'), ('of', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('written', 'VBN'), ('and', 'CC'), ('concerns', 'NNS'), ('events', 'NNS'), ('before', 'IN'), ('9', 'CD'), ('11', 'CD'), ('when', 'WRB'), ('the', 'DT'), ('area', 'NN'), ('was', 'VBD'), ('not', 'RB'), ('established', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('west', 'NN'), ('s', 'JJ'), ('cultural', 'JJ'), ('radar', 'NN'), ('as', 'IN'), ('it', 'PRP'), ('is', 'VBZ'), ('today', 'NN'), ('it', 'PRP'), ('gives', 'VBZ'), ('a', 'DT'), ('view', 'NN'), ('of', 'IN'), ('the', 'DT'), ('region', 'NN'), ('that', 'WDT'), ('is', 'VBZ'), ('uncluttered', 'VBN'), ('by', 'IN'), ('hindsight', 'JJ'), ('reevaluations', 'NNS'), ('kremmer', 'VBP'), ('writes', 'NNS'), ('of', 'IN'), ('his', 'PRP$'), ('time', 'NN'), ('in', 'IN'), ('afghanistan', 'JJ'), ('pakistan', 'NN'), ('tajikstan', 'NN'), ('kashmir', 'NN'), ('and', 'CC'), ('iran', 'NN'), ('giving', 'VBG'), ('us', 'PRP'), ('colorful', 'JJ'), ('and', 'CC'), ('non', 'JJ'), ('journalistic', 'JJ'), ('slices', 'NNS'), ('of', 'IN'), ('life', 'NN'), ('from', 'IN'), ('each', 'DT'), ('region', 'NN'), ('he', 'PRP'), ('enlivens', 'VBZ'), ('his', 'PRP$'), ('writings', 'NNS'), ('with', 'IN'), ('vivid', 'JJ'), ('character', 'NN'), ('studies', 'NNS'), ('of', 'IN'), ('those', 'DT'), ('he', 'PRP'), ('met', 'VBD'), ('on', 'IN'), ('his', 'PRP$'), ('travels', 'NNS'), ('from', 'IN'), ('dignitaries', 'NNS'), ('like', 'IN'), ('ill', 'NN'), ('fated', 'VBN'), ('afghan', 'JJ'), ('dictator', 'NN'), ('mohammed', 'VBD'), ('najibullah', 'JJ'), ('and', 'CC'), ('legendary', 'JJ'), ('guerilla', 'NN'), ('ahmad', 'NN'), ('shah', 'NN'), ('massoud', 'NN'), ('to', 'TO'), ('various', 'JJ'), ('carpet', 'NN'), ('dealers', 'NNS'), ('kremmer', 'VBP'), ('got', 'VBD'), ('to', 'TO'), ('know', 'VB'), ('over', 'IN'), ('his', 'PRP$'), ('time', 'NN'), ('in', 'IN'), ('the', 'DT'), ('region', 'NN'), ('between', 'IN'), ('these', 'DT'), ('character', 'NN'), ('sketches', 'NNS'), ('and', 'CC'), ('kremmer', 'VB'), ('s', 'NN'), ('anecdotes', 'VBZ'), ('he', 'PRP'), ('delivers', 'VBZ'), ('measured', 'JJ'), ('doses', 'NNS'), ('of', 'IN'), ('regional', 'JJ'), ('history', 'NN'), ('and', 'CC'), ('politics', 'NNS'), ('and', 'CC'), ('he', 'PRP'), ('imparts', 'VBZ'), ('a', 'DT'), ('surprising', 'JJ'), ('amount', 'NN'), ('of', 'IN'), ('information', 'NN'), ('about', 'IN'), ('his', 'PRP$'), ('favorite', 'JJ'), ('hobby', 'NN'), ('the', 'DT'), ('asian', 'JJ'), ('carpet', 'NN'), ('the', 'DT'), ('result', 'NN'), ('is', 'VBZ'), ('more', 'JJR'), ('than', 'IN'), ('just', 'RB'), ('some', 'DT'), ('very', 'RB'), ('entertaining', 'JJ'), ('travel', 'NN'), ('writing', 'VBG'), ('kremmer', 'JJR'), ('s', 'NN'), ('lively', 'RB'), ('and', 'CC'), ('discursive', 'JJ'), ('work', 'NN'), ('also', 'RB'), ('functions', 'NNS'), ('as', 'IN'), ('an', 'DT'), ('excellent', 'JJ'), ('introduction', 'NN'), ('to', 'TO'), ('the', 'DT'), ('central', 'JJ'), ('asian', 'JJ'), ('economy', 'NN'), ('and', 'CC'), ('politics', 'NNS'), ('besides', 'IN'), ('being', 'VBG'), ('for', 'IN'), ('those', 'DT'), ('who', 'WP'), ('just', 'RB'), ('like', 'IN'), ('to', 'TO'), ('read', 'VB'), ('about', 'IN'), ('travel', 'NN'), ('in', 'IN'), ('interesting', 'VBG'), ('foreign', 'JJ'), ('parts', 'NNS'), ('the', 'DT'), ('carpet', 'NN'), ('wars', 'NNS'), ('will', 'MD'), ('also', 'RB'), ('be', 'VB'), ('useful', 'JJ'), ('for', 'IN'), ('non', 'JJ'), ('scholars', 'NNS'), ('who', 'WP'), ('want', 'VBP'), ('to', 'TO'), ('have', 'VB'), ('some', 'DT'), ('idea', 'NN'), ('how', 'WRB'), ('movements', 'NNS'), ('like', 'IN'), ('the', 'DT'), ('taliban', 'NN'), ('came', 'VBD'), ('to', 'TO'), ('be', 'VB'), ('but', 'CC'), ('want', 'VBP'), ('to', 'TO'), ('take', 'VB'), ('a', 'DT'), ('spoonful', 'NN'), ('of', 'IN'), ('sugar', 'NN'), ('with', 'IN'), ('this', 'DT'), ('medicine', 'NN'), ('kremmer', 'NN'), ('s', 'NN'), ('book', 'NN'), ('also', 'RB'), ('taught', 'VBD'), ('me', 'PRP'), ('that', 'IN'), ('i', 'NN'), ('ll', 'VBP'), ('never', 'RB'), ('know', 'VBP'), ('enough', 'RB'), ('to', 'TO'), ('bargain', 'VB'), ('effectively', 'RB'), ('for', 'IN'), ('an', 'DT'), ('asian', 'JJ'), ('carpet', 'NN'), ('but', 'CC'), ('his', 'PRP$'), ('rueful', 'NN'), ('and', 'CC'), ('wry', 'NN'), ('work', 'NN'), ('also', 'RB'), ('admits', 'VBZ'), ('that', 'IN'), ('there', 'EX'), ('is', 'VBZ'), ('a', 'DT'), ('certain', 'JJ'), ('pleasure', 'NN'), ('in', 'IN'), ('being', 'VBG'), ('cheated', 'VBN')]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"----Review 2----\\n\")\n",
    "print(\"TokenizedReview:\",df_train_copy.loc[4][\"TokenizedReview\"])\n",
    "print(\"\\n\")\n",
    "print(\"PosTaggedReview:\", df_train_copy.loc[4][\"PosTaggedReview\"])\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "120ce36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Review 3----\n",
      "\n",
      "TokenizedReview: ['please', 'note', 'that', 'this', 'review', 'concerns', 'only', 'the', 'new', 'publications', 'the', 'chronicles', 'of', 'narnia', 'are', 'perfect', 'books', 'they', 'are', 'wonderful', 'for', 'children', 'and', 'adults', 'and', 'can', 'be', 'read', 'again', 'and', 'again', 'c', 's', 'lewis', 'was', 'a', 'brilliant', 'author', 'and', 'theologian', 'and', 'was', 'competent', 'in', 'what', 'he', 'was', 'doing', 'i', 'have', 'been', 'reading', 'these', 'books', 'since', 'i', 'was', 'young', 'enough', 'to', 'pick', 'up', 'a', 'book', 'and', 'i', 'was', 'horrified', 'when', 'i', 'found', 'out', 'they', 'were', 'reprinting', 'them', 'in', 'chronological', 'order', 'why', 'have', 'the', 'publishers', 'decided', 'to', 'tamper', 'with', 'the', 'order', 'reading', 'these', 'books', 'in', 'chronological', 'order', 'spoils', 'all', 'of', 'the', 'surprise', 'and', 'magic', 'out', 'of', 'the', 'first', 'visit', 'to', 'narnia', 'in', 'the', 'lion', 'the', 'witch', 'and', 'the', 'wardrobe', 'because', 'we', 'already', 'know', 'what', 's', 'going', 'on', 'you', 're', 'not', 'supposed', 'to', 'know', 'about', 'the', 'lightpole', 'or', 'who', 'the', 'professor', 'is', 'yet', 'things', 'don', 't', 'always', 'need', 'to', 'be', 'put', 'in', 'chronological', 'order', 'if', 'you', 're', 'going', 'to', 'read', 'them', 'please', 'read', 'them', 'in', 'the', 'correct', 'order', '1', 'the', 'lion', 'the', 'witch', 'and', 'the', 'wardrobe', '2', 'prince', 'caspian', '3', 'the', 'voyage', 'of', 'the', 'dawn', 'treader', '4', 'the', 'silver', 'chair', '5', 'the', 'horse', 'and', 'his', 'boy', '6', 'the', 'magician', 's', 'nephew', 'and', '7', 'the', 'last', 'battle']\n",
      "\n",
      "\n",
      "PosTaggedReview: [('please', 'VB'), ('note', 'NN'), ('that', 'IN'), ('this', 'DT'), ('review', 'NN'), ('concerns', 'VBZ'), ('only', 'RB'), ('the', 'DT'), ('new', 'JJ'), ('publications', 'NNS'), ('the', 'DT'), ('chronicles', 'NNS'), ('of', 'IN'), ('narnia', 'NN'), ('are', 'VBP'), ('perfect', 'JJ'), ('books', 'NNS'), ('they', 'PRP'), ('are', 'VBP'), ('wonderful', 'JJ'), ('for', 'IN'), ('children', 'NNS'), ('and', 'CC'), ('adults', 'NNS'), ('and', 'CC'), ('can', 'MD'), ('be', 'VB'), ('read', 'VBN'), ('again', 'RB'), ('and', 'CC'), ('again', 'RB'), ('c', 'JJ'), ('s', 'NN'), ('lewis', 'NN'), ('was', 'VBD'), ('a', 'DT'), ('brilliant', 'JJ'), ('author', 'NN'), ('and', 'CC'), ('theologian', 'JJ'), ('and', 'CC'), ('was', 'VBD'), ('competent', 'NN'), ('in', 'IN'), ('what', 'WP'), ('he', 'PRP'), ('was', 'VBD'), ('doing', 'VBG'), ('i', 'NN'), ('have', 'VBP'), ('been', 'VBN'), ('reading', 'VBG'), ('these', 'DT'), ('books', 'NNS'), ('since', 'IN'), ('i', 'NN'), ('was', 'VBD'), ('young', 'JJ'), ('enough', 'RB'), ('to', 'TO'), ('pick', 'VB'), ('up', 'RP'), ('a', 'DT'), ('book', 'NN'), ('and', 'CC'), ('i', 'NN'), ('was', 'VBD'), ('horrified', 'VBN'), ('when', 'WRB'), ('i', 'NN'), ('found', 'VBD'), ('out', 'IN'), ('they', 'PRP'), ('were', 'VBD'), ('reprinting', 'VBG'), ('them', 'PRP'), ('in', 'IN'), ('chronological', 'JJ'), ('order', 'NN'), ('why', 'WRB'), ('have', 'VBP'), ('the', 'DT'), ('publishers', 'NNS'), ('decided', 'VBD'), ('to', 'TO'), ('tamper', 'VB'), ('with', 'IN'), ('the', 'DT'), ('order', 'NN'), ('reading', 'VBG'), ('these', 'DT'), ('books', 'NNS'), ('in', 'IN'), ('chronological', 'JJ'), ('order', 'NN'), ('spoils', 'NNS'), ('all', 'DT'), ('of', 'IN'), ('the', 'DT'), ('surprise', 'NN'), ('and', 'CC'), ('magic', 'VB'), ('out', 'IN'), ('of', 'IN'), ('the', 'DT'), ('first', 'JJ'), ('visit', 'NN'), ('to', 'TO'), ('narnia', 'VB'), ('in', 'IN'), ('the', 'DT'), ('lion', 'NN'), ('the', 'DT'), ('witch', 'NN'), ('and', 'CC'), ('the', 'DT'), ('wardrobe', 'NN'), ('because', 'IN'), ('we', 'PRP'), ('already', 'RB'), ('know', 'VBP'), ('what', 'WP'), ('s', 'VBZ'), ('going', 'VBG'), ('on', 'IN'), ('you', 'PRP'), ('re', 'VBP'), ('not', 'RB'), ('supposed', 'VBN'), ('to', 'TO'), ('know', 'VB'), ('about', 'IN'), ('the', 'DT'), ('lightpole', 'NN'), ('or', 'CC'), ('who', 'WP'), ('the', 'DT'), ('professor', 'NN'), ('is', 'VBZ'), ('yet', 'RB'), ('things', 'NNS'), ('don', 'VBP'), ('t', 'NN'), ('always', 'RB'), ('need', 'VB'), ('to', 'TO'), ('be', 'VB'), ('put', 'VBN'), ('in', 'IN'), ('chronological', 'JJ'), ('order', 'NN'), ('if', 'IN'), ('you', 'PRP'), ('re', 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('read', 'VB'), ('them', 'PRP'), ('please', 'VB'), ('read', 'VB'), ('them', 'PRP'), ('in', 'IN'), ('the', 'DT'), ('correct', 'JJ'), ('order', 'NN'), ('1', 'CD'), ('the', 'DT'), ('lion', 'NN'), ('the', 'DT'), ('witch', 'NN'), ('and', 'CC'), ('the', 'DT'), ('wardrobe', 'NN'), ('2', 'CD'), ('prince', 'NN'), ('caspian', 'JJ'), ('3', 'CD'), ('the', 'DT'), ('voyage', 'NN'), ('of', 'IN'), ('the', 'DT'), ('dawn', 'NN'), ('treader', 'NN'), ('4', 'CD'), ('the', 'DT'), ('silver', 'NN'), ('chair', 'NN'), ('5', 'CD'), ('the', 'DT'), ('horse', 'NN'), ('and', 'CC'), ('his', 'PRP$'), ('boy', 'NN'), ('6', 'CD'), ('the', 'DT'), ('magician', 'JJ'), ('s', 'NN'), ('nephew', 'NN'), ('and', 'CC'), ('7', 'CD'), ('the', 'DT'), ('last', 'JJ'), ('battle', 'NN')]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"----Review 3----\\n\")\n",
    "print(\"TokenizedReview:\",df_train_copy.loc[789][\"TokenizedReview\"])\n",
    "print(\"\\n\")\n",
    "print(\"PosTaggedReview:\", df_train_copy.loc[789][\"PosTaggedReview\"])\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a06be45",
   "metadata": {},
   "source": [
    "#### Task 3: Extract unigram features\n",
    "- Extract unigrams from the OriginalReview column in the training set. \n",
    "- Fit the unigrams to the OriginalReview column to generate a feature vector for each review in the training set and testing set. Note that we are using count-based features (i.e., feature values should be the number of times a specific unigram appears in the sentence). \n",
    "- Report the number of features in the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a94f1589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "def ngrammar(review,n = 1):\n",
    "    unigrams_list = []\n",
    "    unigrams_from_reviews = ngrams(review.split(), n)\n",
    "    for item in unigrams_from_reviews:\n",
    "        unigrams_list.append(item)\n",
    "    return unigrams_list\n",
    "        \n",
    "\n",
    "unis = df_train.apply(lambda row : ngrammar(row[\"OriginalReview\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15c2108e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(41,), (years,), (later:,), (the,), (cheese,)...\n",
       "1    [(looking,), (for,), (a,), (louis,), (untermey...\n",
       "2    [(dr.,), (seuss,), (has,), (some,), (really,),...\n",
       "3    [(completly,), (boring!!!,), (yes,), (it's,), ...\n",
       "4    [(the,), (carpet,), (wars,), (is,), (a,), (sam...\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deba09aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(ngram_range=[1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train_og_review = df_train['OriginalReview']\n",
    "test_og_review = df_test['OriginalReview']\n",
    "\n",
    "cv = CountVectorizer(ngram_range = [1,1]) #for unigrams\n",
    "\n",
    "cv.fit(train_og_review)\n",
    "cv.fit(test_og_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb293526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Vector Shape: (12996, 6110)\n",
      "Test Vector Shape: (1602, 6110)\n"
     ]
    }
   ],
   "source": [
    "train_vector = cv.transform(train_og_review)\n",
    "print(\"Train Vector Shape:\",train_vector.shape)\n",
    "\n",
    "test_vector = cv.transform(test_og_review)\n",
    "print(\"Test Vector Shape:\", test_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047399eb",
   "metadata": {},
   "source": [
    "#### Task 4: Train and evaluate classifier\n",
    "- With unigram features generated in step 3, train a Naïve Bayes classifier. \n",
    "- After the training, apply the classifier to the test set and calculate the following performance metrics: \n",
    "    - Overall (average) accuracy, precision, recall and F1 score of the classification system.\n",
    "    - Accuracy, precision, recall and F1 score for each label: Positive, Negative and Neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de14da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "# define true labels from train set\n",
    "x_train = train_vector\n",
    "y_train = df_train[\"overall\"]\n",
    "x_test = test_vector\n",
    "y_test = df_test[\"overall\"]\n",
    "\n",
    "# fit model and test on trai\n",
    "\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6336892c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9101123595505618\n",
      "Precision score:  [0. 0. 1.]\n",
      "Recall score:  [0.         0.         0.91011236]\n",
      "F1 score:  [0.         0.         0.95294118]\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Precision score: \", precision_score(y_test, predictions, average = None))\n",
    "print (\"Recall score: \", recall_score(y_test, predictions, average = None))\n",
    "print (\"F1 score: \", f1_score(y_test, predictions, average = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "872687cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual label performance: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         0\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "    positive       1.00      0.91      0.95      1602\n",
      "\n",
      "    accuracy                           0.91      1602\n",
      "   macro avg       0.33      0.30      0.32      1602\n",
      "weighted avg       1.00      0.91      0.95      1602\n",
      "\n",
      "[[   0    0    0]\n",
      " [   0    0    0]\n",
      " [  67   77 1458]]\n"
     ]
    }
   ],
   "source": [
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe1a6ae",
   "metadata": {},
   "source": [
    "#### Task 5: Add bigram features\n",
    "- Extract bigrams from the OriginalReview column in the training set and add these bigrams into the feature space. \n",
    "- Fit the new features to the OriginalReview column to generate a feature vector for each sentence in the training set and the test set. Report the number of features of the training set and the test set.\n",
    "- Repeat task 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3be1737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammar(review,n = 2):\n",
    "    unigrams_list = []\n",
    "    unigrams_from_reviews = ngrams(review.split(), n)\n",
    "    for item in unigrams_from_reviews:\n",
    "        unigrams_list.append(item)\n",
    "    return unigrams_list\n",
    "        \n",
    "\n",
    "bis = df_train.apply(lambda row : ngrammar(row[\"OriginalReview\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a9161bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(41, years), (years, later:), (later:, the), ...\n",
       "1    [(looking, for), (for, a), (a, louis), (louis,...\n",
       "2    [(dr., seuss), (seuss, has), (has, some), (som...\n",
       "3    [(completly, boring!!!), (boring!!!, yes), (ye...\n",
       "4    [(the, carpet), (carpet, wars), (wars, is), (i...\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "710383e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Vector Shape: (12996, 42128)\n",
      "Train Vector Shape: (1602, 42128)\n"
     ]
    }
   ],
   "source": [
    "cv2 = CountVectorizer(ngram_range = [1,2]) #for bigrams\n",
    "train_og_review_bigrams = df_train['OriginalReview']\n",
    "test_og_review_bigrams = df_test['OriginalReview']\n",
    "\n",
    "cv2.fit(train_og_review_bigrams)\n",
    "cv2.fit(test_og_review_bigrams)\n",
    "\n",
    "\n",
    "train_vector_bigrams = cv2.transform(train_og_review_bigrams)\n",
    "print(\"Train Vector Shape:\",train_vector_bigrams.shape)\n",
    "\n",
    "test_vector_bigrams = cv2.transform(test_og_review_bigrams)\n",
    "print(\"Train Vector Shape:\",test_vector_bigrams.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73c3d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_vector_bigrams\n",
    "y_train = df_train[\"overall\"]\n",
    "x_test = test_vector_bigrams\n",
    "y_test = df_test[\"overall\"]\n",
    "\n",
    "# build model on the training data\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# predict the labels for the test data\n",
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c489c84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9038701622971286\n",
      "Precision score:  [0. 0. 1.]\n",
      "Recall score:  [0.         0.         0.90387016]\n",
      "F1 score:  [0.        0.        0.9495082]\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Precision score: \", precision_score(y_test, predictions, average = None))\n",
    "print (\"Recall score: \", recall_score(y_test, predictions, average = None))\n",
    "print (\"F1 score: \", f1_score(y_test, predictions, average = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "285cd10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual label performance: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         0\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "    positive       1.00      0.90      0.95      1602\n",
      "\n",
      "    accuracy                           0.90      1602\n",
      "   macro avg       0.33      0.30      0.32      1602\n",
      "weighted avg       1.00      0.90      0.95      1602\n",
      "\n",
      "[[   0    0    0]\n",
      " [   0    0    0]\n",
      " [  73   81 1448]]\n"
     ]
    }
   ],
   "source": [
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b48f409",
   "metadata": {},
   "source": [
    "#### Task 6: Add trigram features\n",
    "- Extract trigrams from the OriginalReview column in the training set and add these trigrams into the feature space. \n",
    "- Fit the new features to the OriginalReview column to generate a feature vector for each sentence in the training set and testing set. Report the number of features of the training set and testing set in your documents.\n",
    "- Repeat task 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f01a63a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammar(review,n = 3):\n",
    "    unigrams_list = []\n",
    "    unigrams_from_reviews = ngrams(review.split(), n)\n",
    "    for item in unigrams_from_reviews:\n",
    "        unigrams_list.append(item)\n",
    "    return unigrams_list\n",
    "        \n",
    "tris = df_train.apply(lambda row : ngrammar(row[\"OriginalReview\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd32995d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(41, years, later:), (years, later:, the), (l...\n",
       "1    [(looking, for, a), (for, a, louis), (a, louis...\n",
       "2    [(dr., seuss, has), (seuss, has, some), (has, ...\n",
       "3    [(completly, boring!!!, yes), (boring!!!, yes,...\n",
       "4    [(the, carpet, wars), (carpet, wars, is), (war...\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "716773be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Vector Shape: (12996, 42128)\n",
      "Train Vector Shape: (1602, 99462)\n"
     ]
    }
   ],
   "source": [
    "cv3 = CountVectorizer(ngram_range = [1,3]) #for trigrams\n",
    "train_og_review_trigrams = df_train['OriginalReview']\n",
    "test_og_review_trigrams = df_test['OriginalReview']\n",
    "\n",
    "cv3.fit(train_og_review_trigrams)\n",
    "cv3.fit(test_og_review_trigrams)\n",
    "\n",
    "\n",
    "train_vector_trigrams = cv3.transform(train_og_review_trigrams)\n",
    "print(\"Train Vector Shape:\",train_vector_bigrams.shape)\n",
    "\n",
    "test_vector_trigrams = cv3.transform(test_og_review_trigrams)\n",
    "print(\"Train Vector Shape:\",test_vector_trigrams.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6652800",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_vector_trigrams\n",
    "y_train = df_train[\"overall\"]\n",
    "x_test = test_vector_trigrams\n",
    "y_test = df_test[\"overall\"]\n",
    "\n",
    "# build model on the training data\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# predict the labels for the test data\n",
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "469943d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9038701622971286\n",
      "Precision score:  [0. 0. 1.]\n",
      "Recall score:  [0.         0.         0.90387016]\n",
      "F1 score:  [0.        0.        0.9495082]\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Precision score: \", precision_score(y_test, predictions, average = None))\n",
    "print (\"Recall score: \", recall_score(y_test, predictions, average = None))\n",
    "print (\"F1 score: \", f1_score(y_test, predictions, average = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21356a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual label performance: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         0\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "    positive       1.00      0.90      0.95      1602\n",
      "\n",
      "    accuracy                           0.90      1602\n",
      "   macro avg       0.33      0.30      0.32      1602\n",
      "weighted avg       1.00      0.90      0.95      1602\n",
      "\n",
      "[[   0    0    0]\n",
      " [   0    0    0]\n",
      " [  92   62 1448]]\n"
     ]
    }
   ],
   "source": [
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a646393",
   "metadata": {},
   "source": [
    "#### Task 7: Add TF-IDF features\n",
    "- Among the models trained by features in tasks 3, 5 or 6 (unigram, unigram + bigram, unigram + bigram + trigram, respectively) choose the best performing model (based on overall F1 score) and substitute count-based features with TF-IDF features. \n",
    "- Repeat task 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9bedaa0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12996, 35910) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# this time, we vectorize using TF-IDF\n",
    "train_og_review_tfidf = df_train['OriginalReview']\n",
    "test_og_review_tfidf = df_test['OriginalReview']\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "tf.fit(train_og_review_tfidf)\n",
    "\n",
    "# encode document\n",
    "data = tf.transform(train_og_review_tfidf)\n",
    "\n",
    "# summarize encoded vector\n",
    "print(data.shape,\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "502ee9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12996, 35910) \n",
      "\n",
      "(1602, 35910) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_tfidf = tf.fit_transform(train_og_review_tfidf)\n",
    "print(train_data_tfidf.shape,\"\\n\") \n",
    "\n",
    "test_data_tfidf = tf.transform(test_og_review_tfidf)\n",
    "print(test_data_tfidf.shape,\"\\n\") \n",
    "\n",
    "idf = tf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb596918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.966916354556804\n",
      "Precision score:  [0. 0. 1.]\n",
      "Recall score:  [0.         0.         0.96691635]\n",
      "F1 score:  [0.         0.         0.98317994]\n"
     ]
    }
   ],
   "source": [
    "# define true labels from train set\n",
    "x_train = train_data_tfidf\n",
    "y_train = df_train[\"overall\"]\n",
    "x_test = test_data_tfidf\n",
    "y_test = df_test[\"overall\"]\n",
    "\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Precision score: \", precision_score(y_test, predictions, average = None))\n",
    "print (\"Recall score: \", recall_score(y_test, predictions, average = None))\n",
    "print (\"F1 score: \", f1_score(y_test, predictions, average = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98c2e5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual label performance: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         0\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "    positive       1.00      0.97      0.98      1602\n",
      "\n",
      "    accuracy                           0.97      1602\n",
      "   macro avg       0.33      0.32      0.33      1602\n",
      "weighted avg       1.00      0.97      0.98      1602\n",
      "\n",
      "[[   0    0    0]\n",
      " [   0    0    0]\n",
      " [  16   37 1549]]\n"
     ]
    }
   ],
   "source": [
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efbd0a0",
   "metadata": {},
   "source": [
    "#### Task 8:  Train models with other columns\n",
    "- Among the models trained by features in tasks 3, 5, 6 or 7, choose the best performing feature set, and train Naïve Bayes classifiers on the following columns: **CleanedReview, StopwordRemovedReview, StemmedReview** and record the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2369ea04",
   "metadata": {},
   "source": [
    "##### For cleanedReview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80a3ac49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12996, 35837) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this time, we vectorize using TF-IDF\n",
    "train_clean_review_tfidf = df_train['CleanedReview']\n",
    "test_clean_review_tfidf = df_test['CleanedReview']\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "tf.fit(train_clean_review_tfidf)\n",
    "\n",
    "# encode document\n",
    "data = tf.transform(train_clean_review_tfidf)\n",
    "\n",
    "# summarize encoded vector\n",
    "print(data.shape,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e04632a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12996, 35837) \n",
      "\n",
      "(1602, 35837) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_tfidf = tf.fit_transform(train_clean_review_tfidf)\n",
    "print(train_data_tfidf.shape,\"\\n\") \n",
    "\n",
    "test_data_tfidf = tf.transform(test_clean_review_tfidf)\n",
    "print(test_data_tfidf.shape,\"\\n\") \n",
    "\n",
    "idf = tf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea53a6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9675405742821473\n",
      "Precision score:  [0. 0. 1.]\n",
      "Recall score:  [0.         0.         0.96754057]\n",
      "F1 score:  [0.         0.         0.98350254]\n"
     ]
    }
   ],
   "source": [
    "# define true labels from train set\n",
    "x_train = train_data_tfidf\n",
    "y_train = df_train[\"overall\"]\n",
    "x_test = test_data_tfidf\n",
    "y_test = df_test[\"overall\"]\n",
    "\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Precision score: \", precision_score(y_test, predictions, average = None))\n",
    "print (\"Recall score: \", recall_score(y_test, predictions, average = None))\n",
    "print (\"F1 score: \", f1_score(y_test, predictions, average = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19f530f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual label performance: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         0\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "    positive       1.00      0.97      0.98      1602\n",
      "\n",
      "    accuracy                           0.97      1602\n",
      "   macro avg       0.33      0.32      0.33      1602\n",
      "weighted avg       1.00      0.97      0.98      1602\n",
      "\n",
      "[[   0    0    0]\n",
      " [   0    0    0]\n",
      " [  16   36 1550]]\n"
     ]
    }
   ],
   "source": [
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39cc840",
   "metadata": {},
   "source": [
    "##### For StopwordRemovedReview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c54f0077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12996, 35691) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this time, we vectorize using TF-IDF\n",
    "train_swr_review_tfidf = df_train['StopwordRemovedReview']\n",
    "test_swr_review_tfidf = df_test['StopwordRemovedReview']\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "tf.fit(train_swr_review_tfidf)\n",
    "\n",
    "# encode document\n",
    "data = tf.transform(train_swr_review_tfidf)\n",
    "\n",
    "# summarize encoded vector\n",
    "print(data.shape,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98fcbb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12996, 35691) \n",
      "\n",
      "(1602, 35691) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_tfidf = tf.fit_transform(train_swr_review_tfidf)\n",
    "print(train_data_tfidf.shape,\"\\n\") \n",
    "\n",
    "test_data_tfidf = tf.transform(test_swr_review_tfidf)\n",
    "print(test_data_tfidf.shape,\"\\n\") \n",
    "\n",
    "idf = tf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e1596f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9681647940074907\n",
      "Precision score:  [0. 0. 1.]\n",
      "Recall score:  [0.         0.         0.96816479]\n",
      "F1 score:  [0.         0.         0.98382493]\n"
     ]
    }
   ],
   "source": [
    "# define true labels from train set\n",
    "x_train = train_data_tfidf\n",
    "y_train = df_train[\"overall\"]\n",
    "x_test = test_data_tfidf\n",
    "y_test = df_test[\"overall\"]\n",
    "\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Precision score: \", precision_score(y_test, predictions, average = None))\n",
    "print (\"Recall score: \", recall_score(y_test, predictions, average = None))\n",
    "print (\"F1 score: \", f1_score(y_test, predictions, average = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f2275166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual label performance: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         0\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "    positive       1.00      0.97      0.98      1602\n",
      "\n",
      "    accuracy                           0.97      1602\n",
      "   macro avg       0.33      0.32      0.33      1602\n",
      "weighted avg       1.00      0.97      0.98      1602\n",
      "\n",
      "[[   0    0    0]\n",
      " [   0    0    0]\n",
      " [  17   34 1551]]\n"
     ]
    }
   ],
   "source": [
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff368af7",
   "metadata": {},
   "source": [
    "##### For StemmedReview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0990c407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12996, 23155) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this time, we vectorize using TF-IDF\n",
    "train_stem_review_tfidf = df_train['StemmedReview']\n",
    "test_stem_review_tfidf = df_test['StemmedReview']\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "tf.fit(train_stem_review_tfidf)\n",
    "\n",
    "# encode document\n",
    "data = tf.transform(train_stem_review_tfidf)\n",
    "\n",
    "# summarize encoded vector\n",
    "print(data.shape,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "528d723d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12996, 23155) \n",
      "\n",
      "(1602, 23155) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_tfidf = tf.fit_transform(train_stem_review_tfidf)\n",
    "print(train_data_tfidf.shape,\"\\n\") \n",
    "\n",
    "test_data_tfidf = tf.transform(test_stem_review_tfidf)\n",
    "print(test_data_tfidf.shape,\"\\n\") \n",
    "\n",
    "idf = tf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f856a010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9625468164794008\n",
      "Precision score:  [0. 0. 1.]\n",
      "Recall score:  [0.         0.         0.96254682]\n",
      "F1 score:  [0.         0.         0.98091603]\n"
     ]
    }
   ],
   "source": [
    "# define true labels from train set\n",
    "x_train = train_data_tfidf\n",
    "y_train = df_train[\"overall\"]\n",
    "x_test = test_data_tfidf\n",
    "y_test = df_test[\"overall\"]\n",
    "\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "print (\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "print (\"Precision score: \", precision_score(y_test, predictions, average = None))\n",
    "print (\"Recall score: \", recall_score(y_test, predictions, average = None))\n",
    "print (\"F1 score: \", f1_score(y_test, predictions, average = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc95a86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual label performance: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         0\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "    positive       1.00      0.96      0.98      1602\n",
      "\n",
      "    accuracy                           0.96      1602\n",
      "   macro avg       0.33      0.32      0.33      1602\n",
      "weighted avg       1.00      0.96      0.98      1602\n",
      "\n",
      "[[   0    0    0]\n",
      " [   0    0    0]\n",
      " [  18   42 1542]]\n"
     ]
    }
   ],
   "source": [
    "print (\"Individual label performance: \")\n",
    "print (classification_report(y_test, predictions))\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a262f4d4",
   "metadata": {},
   "source": [
    "#### Task 9:  Interpret and discuss classification results\n",
    "- Discuss the input (original column) and feature combination that yields the best performance with Naïve Bayes algorithm. Why do you think this combination works better than the others? \n",
    "- For the best-performing model, examine the performance for each label. How do they compare to one another? What can be inferred from these results? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06834550",
   "metadata": {},
   "source": [
    "*Among the unigrams, unigrams+ bigrams, unigrams + bigrams + trigrams, and tf-idf, we find that **tf-idf** gives the best performance. This can be clearly explained if we dig deeper into how CountVectorizer and TF-IDF works*. \n",
    "\n",
    "*CountVectorizers just count the number of times words in the reviews are repeated. It's a way to convert them into frequency statistically. Since this only counts, we cannot stress on which words are more important and which of them can be discarded. Higher the number of times words are repeated in a document, more importance is assigned to them just purely based on frequency*.\n",
    "\n",
    "*On the contrary, TF-IDF assigns numerical importance to words by calculating frequency and adjusting this entity using log scale of occurence in documents. Hence words that appear in high frequency in a single document but doesn't happen in rest of the documents will reduce its overall numerical importance*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b540e6bf",
   "metadata": {},
   "source": [
    "*For TF_IDF, we can see that we achieve accuracy of 96.7 % for positive label. Since the test data doesn't have any negative and neutral labels, it is hard to compare three labels to one another. But we see that precision for positive label is 1. This is impossible in real life scenario. We can deduce that if test data has only positive labels, chances of review being incorrectly labelled is zero. Yet somehow, we see that 3.3% of reviews were incorrectly labelled.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb5afb",
   "metadata": {},
   "source": [
    "#### Task 10: Error Analysis\n",
    "- For each label, print out 3 examples (from the test set) that were misclassified by the best-performing model. \n",
    "- Discuss your thoughts about why these examples might have been misclassified. Based on these examples, what can be done to improve the model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2e01f43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample (20, \"this was a touching story, wonderful book and hard to put down.  from start to end the story will make you want to read more by this author.  five stars wasn't enough!\")  has been classified as neutral and should be positive\n",
      "\n",
      "\n",
      "Sample (46, 'i loved this book. i chose the five star review because this book was the kind of book that i would want to read over and over. i hope that there a second book in the series. it would be a really good idea to consider. hope that many other readers will be able to enjoy this book as much as i did. five star rating is a very hard rating to get an i feel this book deserve it.')  has been classified as neutral and should be positive\n",
      "\n",
      "\n",
      "Sample (75, 'unimaginable though could not put it down')  has been classified as negative and should be positive\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = predictions\n",
    "labels = df_test[\"overall\"]\n",
    "inputs = df_test[\"OriginalReview\"]\n",
    "\n",
    "counter = 0\n",
    "for idx, prediction, label in zip(enumerate(inputs), predictions, labels):\n",
    "    if counter < 3:\n",
    "        if prediction != label:\n",
    "            print(\"Sample\", idx, ' has been classified as', prediction, 'and should be', label) \n",
    "            print (\"\\n\")\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f83b88",
   "metadata": {},
   "source": [
    "*In sample number 20, there are words like wonderful, hard, down, more and wasn't, which approximately lead to believe that this review is a neutral review*. \n",
    "\n",
    "*In sample 46, there are positive words like loved, good, enjoy and a negative word-hard. But compared to length of the review, these words are not repeated enough throughout and majority of words are neutral. This might have lead to review being incorrectly labelled as neutral*.\n",
    "\n",
    "*Sample 75 can be clearly seen as negative if not read by human. Prefix 'un' and 'not' can be confusing for computer and hence was labelled as negative instead of positive.*\n",
    "\n",
    "*One way to improve accuracy can be to add a negation before every word. This will eliminate labels that incorrectly classified as negative. This should also help with neutral reviews but could classify positive reviews incorrectly.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670e1831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
